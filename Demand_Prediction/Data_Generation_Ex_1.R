library(tidyverse)
library(corrplot)

# Load anonymized cluster information
load("Cluster_Information.RData")

# Set seed for consistency
set.seed(761)

# Choose necessary columns, as well as some regression columns
clusters = clusters[,colnames(clusters) %in% c("cluster_id","m_factor","beta","pop_size_b0","pop_size_b1","pop_size_b2","pop_size_b3","beta","m_factor",
                                               "Given_Birth_Female_12_19","HH_Main_Info_Community","HH_Depend_Subsistence","density")]

# Randomly generate the underlying demand parameters
true_parameters = data.frame(cluster_id = clusters$cluster_id)
reg_cols = colnames(clusters)[!colnames(clusters) %in% c("cluster_id","m_factor","beta","pop_size_b0","pop_size_b1","pop_size_b2","pop_size_b3")]

# p is generated as a linear model
p_coefs = rnorm(4, mean = 0, sd = 1) / as.numeric(colSums(clusters[,reg_cols]))/nrow(clusters)
p_coefs = (as.matrix(clusters[,reg_cols[c(1,2,3,4)]]) %*% p_coefs)[,1]
p_coefs = p_coefs - min(p_coefs)
true_parameters$p = p_coefs / max(p_coefs) * 4e-3 + 1e-3

# q is generated as a piecewise combination of two linear models. The combination is based on the first variable
q_coefs = rnorm(3, mean = 0, sd = 1) / as.numeric(colSums(clusters[,reg_cols[c(2,3,4)]]))/nrow(clusters)
q_coefs = (as.matrix(clusters[,reg_cols[c(2,3,4)]]) %*% q_coefs)[,1] 
q_coefs = q_coefs - min(q_coefs)
median_col = quantile(clusters[[reg_cols[1]]],0.5,na.rm=T)
q_coefs = ifelse(clusters[[reg_cols[1]]] < median_col, q_coefs, q_coefs + 2*mean(q_coefs))
true_parameters$q = q_coefs / max(q_coefs) * 1e-1 + 2e-2

# r is generated as a linear model
r_coefs = rnorm(4, mean = 0, sd = 1) / as.numeric(colSums(clusters[,reg_cols]))/nrow(clusters)
r_coefs = (as.matrix(clusters[,reg_cols[c(1,2,3,4)]]) %*% r_coefs)[,1] 
r_coefs = r_coefs - min(r_coefs)
true_parameters$r = r_coefs / max(r_coefs) * 7/120 + 1/120
true_parameters$m = clusters$m_factor*(clusters$pop_size_b0 + true_parameters$r * clusters$pop_size_b1 + true_parameters$r^2 * clusters$pop_size_b2 + true_parameters$r^3 * clusters$pop_size_b3)
true_parameters$m = round(true_parameters$m)
true_parameters$m = ifelse(true_parameters$m < 10, 10, true_parameters$m)

# c is generated by a linear model, as a fraction of the density at 5 km (corrected by a minimum m_factor)
# For the linear model, use variables 3 and 5
c_coefs = rnorm(3, mean = 0, sd = 3) / as.numeric(colSums(clusters[,reg_cols[c(1,2,3)]]))/nrow(clusters)
c_coefs = (as.matrix(clusters[,reg_cols[c(1,2,3)]]) %*% c_coefs)[,1]
c_coefs = c_coefs - min(c_coefs)
c_coefs = c_coefs / (3*max(c_coefs))
true_parameters$c = clusters$density * pi*(5/120)^2 * min(clusters$m_factor) * c_coefs
true_parameters$c = ifelse(true_parameters$c / true_parameters$m > 0.9, true_parameters$m*runif(nrow(true_parameters),min=0.8,max=0.9), true_parameters$c)
true_parameters$c = round(true_parameters$c)

# Generate visits to clusters: 50% get training visits above the inflection point, 50% below.
# No site can have more than 60 visits (in effect, 43% have training visits above the inflection point)
# Demand is generated by the Bass model, and scaled with a (+/-) error of up to 10%
# We also generate total existing client demands per visit, as a random variable around 1% of c per training visit
visits = data.frame(cluster_id = numeric(), cluster_visit_id = numeric(), demand = numeric(), training = numeric())
count = 1
for(i in 1:nrow(clusters)){
  p = true_parameters$p[i]
  q = true_parameters$q[i]
  m = true_parameters$m[i]
  c = true_parameters$c[i]
  theta = (m - c)*p/(m*p+c*q)
  inflection = pmax(log((m - c)/(m*p+c*q)*q)/(p+q),0)
  if(runif(1) > 0.5){
    clusters[i,"training_visits"] = sample(max(1,min(round(inflection),60)):max(1,min(2*round(inflection),60)),1) # Check change here in PGB (min 1)
  } else{
    clusters[i,"training_visits"] = sample(1:min(round(inflection),60),1)
  }
  clusters[i,"visits_to_cluster"] = max(clusters[i,"training_visits"],round(clusters[i,"training_visits"]*(1.25+runif(1,min=-0.2,max=0.2))))
  clusters[i,"ex_client_per_visit"] = runif(1,min=0,max=0.02) * c # Check change here in PGB (now per visit, was total)
  
  for(j in 1:clusters[i,"visits_to_cluster"]){
    demand = (1-theta*exp(-(p+q)*j))/(1+q/p*theta*exp(-(p+q)*j)) - (1-theta*exp(-(p+q)*(j-1)))/(1+q/p*theta*exp(-(p+q)*(j-1)))
    demand = demand * m * runif(1,min=0.9,max=1.1)
    visits[count,"cluster_id"] = clusters$cluster_id[i]
    visits[count,"cluster_visit_id"] = j
    visits[count,"demand"] = round(demand)
    visits[count,"training"] = ifelse(j <= clusters$training_visits[i],1,0)
    count = count + 1
  }
}

# Visualize
hist(true_parameters$r*120,breaks=100) # in km
hist(true_parameters$p,breaks=100)
hist(true_parameters$q,breaks=100)
hist(true_parameters$m,breaks=100)
hist(true_parameters$c,breaks=100)
hist(true_parameters$c/true_parameters$m,breaks=100)
clusters$inflection = pmax(log((true_parameters$m - true_parameters$c)/(true_parameters$m*true_parameters$p+true_parameters$c*true_parameters$q)*true_parameters$q)/(true_parameters$p+true_parameters$q),0)
mean(clusters$inflection < clusters$training_visits)
summary(clusters$inflection)
clusters$inflection = NULL
corrplot(cor(true_parameters[,!colnames(true_parameters) %in% c("cluster_id")]))

# Save data
rm(list=setdiff(ls(), c("clusters","visits","true_parameters")))
save(clusters,visits,true_parameters,file="Data_Ex_1.RData")
